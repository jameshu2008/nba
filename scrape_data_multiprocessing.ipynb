{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Beautiful Soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# from urllib.request import urlopen\n",
    "# from bs4 import BeautifulSoup\n",
    "# import pandas as pd\n",
    "\n",
    "# url = \"http://history.basketballmonster.com/Season?seasonId=51&cats=9\"\n",
    "# page = urlopen(url)\n",
    "# soup = BeautifulSoup(page, \"html.parser\")\n",
    "\n",
    "# print(soup.prettify())\n",
    "\n",
    "# table = soup.find(\"table\", attrs={\"class\":\"seasonDetailsT\"})\n",
    "\n",
    "# table.prettify()\n",
    "\n",
    "# data = []\n",
    "# rows = table.find_all('tr')\n",
    "# for row in rows:\n",
    "#     cols = row.find_all('td') or row.find_all('label') \n",
    "#     cols = [ele.text.strip() for ele in cols]\n",
    "#     data.append([ele for ele in cols if ele])\n",
    "# df = pd.DataFrame(data)\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from urllib\n",
    "# from bs4 import BeautifulSoup\n",
    "\n",
    "# leads = [\n",
    "#   (\"http://history.basketballmonster.com/Season?seasonId=18&cats=9\",\n",
    "#    \"2009-2010\"),\n",
    "#   (\"http://history.basketballmonster.com/Season?seasonId=27&cats=9\",\n",
    "#    \"2010-2011\"),\n",
    "#   (\"http://history.basketballmonster.com/Season?seasonId=32&cats=9\",\n",
    "#    \"2011-2012\"),\n",
    "#   (\"http://history.basketballmonster.com/Season?seasonId=36&cats=9\",\n",
    "#    \"2012-2013\"),\n",
    "#   (\"http://history.basketballmonster.com/Season?seasonId=44&cats=9\",\n",
    "#    \"2013-2014\"),\n",
    "#   (\"http://history.basketballmonster.com/Season?seasonId=50&cats=9\",\n",
    "#    \"2014-2015\"),\n",
    "#   (\"http://history.basketballmonster.com/Season?seasonId=51&cats=9\",\n",
    "#    \"2015-2016\")\n",
    "# ]\n",
    "\n",
    "# results = pd.DataFrame()\n",
    "\n",
    "# for lead in leads:\n",
    "    \n",
    "#   url = lead[0]\n",
    "#   season = lead[1]\n",
    "\n",
    "#   # Read all tables on url, use row 0 as the headers \n",
    "#   df = pd.read_html(url, header=0) \n",
    "\n",
    "#   # Drop table 0, unneeded\n",
    "#   df = df[1]\n",
    "\n",
    "#   # Lower-case all column names\n",
    "#   df.columns = map(str.lower, df.columns)\n",
    "#   df.columns = map(str.strip, df.columns)\n",
    "  \n",
    "#   # Drop repeated header rows, 2nd method prefered\n",
    "#   # df = df.drop_duplicates(keep=False)\n",
    "#   # df = df[df.player != 'player']\n",
    "  \n",
    "#   # Add a new column to record the season\n",
    "#   df['season'] = pd.Series()\n",
    "#   df['season'] = season\n",
    "  \n",
    "#   # Print general info\n",
    "#   print(df.shape)\n",
    "    \n",
    "#   if results.empty:\n",
    "#     results = df\n",
    "#   else:\n",
    "#     results = results.append(df)\n",
    "\n",
    "# print(results.shape)\n",
    "# print(results.columns)\n",
    "# results.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Stats from Basketball-Reference.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definitions & Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import urllib\n",
    "import pickle\n",
    "import json\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from google import search\n",
    "from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save/load dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "test = {\n",
    "    'words': \"\"\"\n",
    "        Lorem ipsum dolor sit amet, consectetur adipiscing \n",
    "        elit. Mauris adipiscing adipiscing placerat. \n",
    "        Vestibulum augue augue, \n",
    "        pellentesque quis sollicitudin id, adipiscing.\n",
    "        \"\"\",\n",
    "    'list': list(range(10000)),\n",
    "    'dict': dict((str(i),'a') for i in range(10000)),\n",
    "    'int': 100,\n",
    "    'float': 100.123456\n",
    "}\n",
    "\n",
    "def sizeof_fmt(num, suffix='B'):\n",
    "    for unit in ['','Ki','Mi','Gi','Ti','Pi','Ei','Zi']:\n",
    "        if abs(num) < 1024.0:\n",
    "            return \"%3.1f %s%s\" % (num, unit, suffix)\n",
    "        num /= 1024.0\n",
    "    return \"%.1f %s%s\" % (num, 'Yi', suffix)\n",
    "\n",
    "def get_file_size(filename):\n",
    "    statinfo = os.stat(filename)\n",
    "    return sizeof_fmt(statinfo.st_size)\n",
    "\n",
    "def save_dict(dictionary, filename):\n",
    "    with open(filename, 'wb') as file:\n",
    "        pickle.dump(dictionary, file)\n",
    "    print('\\n', 'file size: ', get_file_size(filename))\n",
    "    return True;\n",
    "\n",
    "def load_dict(filename):\n",
    "    print('\\n', 'file size: ', get_file_size(filename))\n",
    "    with open(filename, 'rb') as file:\n",
    "        return pickle.load(file)\n",
    "\n",
    "save_dict(test, 'test.pickle')\n",
    "\n",
    "len(load_dict('test.pickle'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define & Load Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Tables to retrieve for each player, by table html ids\n",
    "table_ids = [\n",
    "  'per_game',\n",
    "  'totals',\n",
    "  'per_minute', # per 36 minutes\n",
    "  'per_poss', # per 100 possessions\n",
    "  'advanced', # advanced\n",
    "    \n",
    "  'playoffs_per_game',\n",
    "  'playoffs_totals',\n",
    "  'playoffs_per_minute', # playoffs per 36 minutes\n",
    "  'playoffs_per_poss', # playoffs per 100 possessions\n",
    "  'playoffs_advanced', \n",
    "    \n",
    "  'all_star',\n",
    "  'all_college_stats',\n",
    "  'all_salaries',\n",
    "]\n",
    "\n",
    "test_names = ['Michael Jordan', 'Vince Carter', 'Yao Ming']\n",
    "test_urls = [\n",
    "    \"https://www.basketball-reference.com/players/m/moncrsi01.html\",\n",
    "    \"https://www.basketball-reference.com/players/b/bellawa01.html\",\n",
    "]\n",
    "test_data = load_dict('data_outputs_temp.pickle')\n",
    "test_keys = list(test_data.keys())[0:3]\n",
    "test_list = [{key: test_data[key]} for key in test_keys]\n",
    "\n",
    "\n",
    "# save_dict(hall_of_famers, 'hall_of_famers.pickle')\n",
    "# save_dict(retired_all_stars, 'retired_all_stars.pickle')\n",
    "# save_dict(retired_all_nbas, 'retired_all_nbas.pickle')\n",
    "\n",
    "# Load list of player (names) \n",
    "hall_of_famers = load_dict('hall_of_famers.pickle')\n",
    "retired_all_stars = load_dict('retired_all_stars.pickle')\n",
    "retired_all_nbas = load_dict('retired_all_nbas.pickle')\n",
    "players_2015_2016 = load_dict('players_2015_2016.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get stats URL for a player name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def get_url_title(url):\n",
    "    page = urllib.request.urlopen(url)\n",
    "    soup = BeautifulSoup(page, \"html.parser\")\n",
    "    return soup.title.text\n",
    "\n",
    "def get_url(player_name):       \n",
    "    query = (\n",
    "        'site:www.basketball-reference.com/players/*/*.html '\n",
    "        '{player_name} Stats').format(player_name=player_name)\n",
    "    print('query: ', query)\n",
    "\n",
    "    results = search(query=query, start=0, stop=1)\n",
    "    urls = list(results)        \n",
    "\n",
    "    if urls:\n",
    "        return {player_name: urls[0]}\n",
    "    else:\n",
    "        print('url found: None')\n",
    "        \n",
    "        \n",
    "get_url(test_names[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get URLs for a list of player names, MULTIPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def get_urls(player_names, num_processes):\n",
    "    p = Pool(processes=num_processes)\n",
    "    outputs = p.map(get_url, player_names)\n",
    "    p.close()\n",
    "    return outputs\n",
    "\n",
    "print(get_urls(test_names[0:2], 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get stats table for an URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def get_table(url):\n",
    "    \n",
    "    output = {}\n",
    "    \n",
    "    page = urllib.request.urlopen(url)\n",
    "    urlHtml = page.read().decode()\n",
    "\n",
    "    # Get the player name\n",
    "    soup = BeautifulSoup(urlHtml, \"html.parser\")\n",
    "    player_name = soup.find(\"h1\").text\n",
    "\n",
    "    # Set the url\n",
    "    output.setdefault(player_name, {}).setdefault('url', url);\n",
    "\n",
    "    # Uncomment the tables\n",
    "    uncommentedUrlHtml = urlHtml.replace('-->', '')\n",
    "    uncommentedUrlHtml = uncommentedUrlHtml.replace('<!--', '')\n",
    "\n",
    "    for table_id in table_ids:\n",
    "        list_of_df = []\n",
    "        try:\n",
    "            list_of_df = pd.read_html(\n",
    "                uncommentedUrlHtml, \n",
    "                header=0, \n",
    "                attrs={'id': table_id})\n",
    "        except ValueError as err:\n",
    "            # Set failures\n",
    "            output.setdefault(player_name, {}).setdefault('failures', {}).update({table_id: str(err)})\n",
    "            continue;\n",
    "\n",
    "        # Drop 'Unnamed' columns\n",
    "        for df in list_of_df:\n",
    "          df.drop([col_name for col_name in df.columns if 'Unnamed' in col_name], axis=1, inplace=True)\n",
    "\n",
    "        # Set table\n",
    "        output.setdefault(player_name, {}).setdefault('tables', {}).update({table_id: list_of_df[0]})\n",
    "\n",
    "    # Print processing info\n",
    "    print(player_name)\n",
    "    print('Tables Found: ', len(output[player_name].get('tables', {})), \n",
    "          ' | Failures: ', output[player_name].get('failures', {}).keys())\n",
    "    print(' ')\n",
    "\n",
    "    return output\n",
    "\n",
    "print(get_table(test_urls[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get stats tables for a list of URLs, MULTIPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from multiprocessing import Pool\n",
    "from functools import partial\n",
    "\n",
    "def get_tables(urls, num_processes):\n",
    "    p = Pool(processes=num_processes)\n",
    "    outputs = p.map(get_table, urls)\n",
    "    p.close()\n",
    "    return outputs\n",
    "\n",
    "len(get_tables(test_urls, 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Utility function to merge retrived data tables into 1 dictionary.\n",
    "def merge_dict(list_of_dict):\n",
    "    merged_dict = {}\n",
    "    for dictionary in list_of_dict:\n",
    "        merged_dict.update(dictionary)\n",
    "    return merged_dict\n",
    "\n",
    "print(merge_dict(test_list).keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# players_2015_2016_data = merge_dict(outputs)\n",
    "# len(players_2015_2016_data)\n",
    "# players_2015_2016_data['LeBron James']['tables']['all_star']\n",
    "# save_dict(players_2015_2016_data, 'players_2015_2016_data.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
