{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Stats from Basketball-Reference.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports, Constants, Utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 31 µs, sys: 1e+03 ns, total: 32 µs\n",
      "Wall time: 35 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import datetime\n",
    "\n",
    "import json\n",
    "import pickle\n",
    "import urllib\n",
    "import pandas as pd\n",
    "import google\n",
    "import random\n",
    "import time\n",
    "import multiprocessing\n",
    "import gspread\n",
    "import unidecode\n",
    "\n",
    "from gspread import WorksheetNotFound\n",
    "from bs4 import BeautifulSoup\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "\n",
    "DATETIME_STRING_FORMAT = '%Y-%m-%d %H:%M:%S'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Spreadsheets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOADED > 100 rows from spreadsheet: \"test_spreadsheet\" | worksheet: \"test\" | timestamp: 2017-11-21 13:04:02 \n",
      "\n",
      "         0\n",
      "0  michale\n",
      "1     kobe\n",
      "2        0\n",
      "3        1\n",
      "4        2\n"
     ]
    }
   ],
   "source": [
    "def load_list_from_worksheet(spreadsheet_name, worksheet_name):\n",
    "  \n",
    "    scope = ['https://spreadsheets.google.com/feeds']\n",
    "    credentials = ServiceAccountCredentials.from_json_keyfile_name('Data-35df9a696bc1.json', scope)\n",
    "    gc = gspread.authorize(credentials)\n",
    "\n",
    "    spreadsheet = gc.open(spreadsheet_name)\n",
    "    worksheet = spreadsheet.worksheet(worksheet_name)\n",
    "\n",
    "    rows = worksheet.get_all_values()\n",
    "    \n",
    "    first_row = rows[0]\n",
    "    first_cell = first_row[0]\n",
    "    \n",
    "    try:\n",
    "        timestamp = datetime.datetime.strptime(first_cell, DATETIME_STRING_FORMAT)\n",
    "        rows.remove(first_row)\n",
    "    except ValueError:\n",
    "        timestamp = None\n",
    "\n",
    "    print(\n",
    "        'LOADED > {num_rows} rows from '\n",
    "        'spreadsheet: \"{spreadsheet_name}\" | '\n",
    "        'worksheet: \"{worksheet_name}\" | '\n",
    "        'timestamp: {timestamp}'.format(\n",
    "            num_rows=len(rows), spreadsheet_name=spreadsheet_name, \n",
    "            worksheet_name=worksheet_name, timestamp=timestamp), '\\n')\n",
    "\n",
    "    df = pd.DataFrame.from_records(rows)\n",
    "    \n",
    "    return df\n",
    "\n",
    "worksheet = load_list_from_worksheet('test_spreadsheet', 'test')\n",
    "\n",
    "print(worksheet.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List sanitized! Length:  4 \n",
      "\n",
      "['shaquille oneal', 'j j reddick', 'vince carter', 'kobe bryant']\n"
     ]
    }
   ],
   "source": [
    "def sanitize_list(raw_list):\n",
    "    # Remove accented (Spanish) characters.\n",
    "    sanitized_list = [unidecode.unidecode(accented_string) for accented_string in raw_list]\n",
    "    # Trim & lower-case\n",
    "    sanitized_list = [string.strip().lower() for string in sanitized_list]\n",
    "    # Remove quotes\n",
    "    sanitized_list = [string.replace(\"'\", \"\") for string in sanitized_list]\n",
    "    sanitized_list = [string.replace('\"', '') for string in sanitized_list]\n",
    "    # Remove dots\n",
    "    sanitized_list = [string.replace('.', '') for string in sanitized_list]\n",
    "    for i, string in enumerate(sanitized_list):\n",
    "        if \",\" in string:\n",
    "            lst = string.split(\",\")\n",
    "            lst.reverse()\n",
    "            lst = [token.strip() for token in lst]\n",
    "            sanitized_string = \" \".join(lst)\n",
    "            sanitized_list[i] = sanitized_string\n",
    "    \n",
    "    print('List sanitized! Length: ', len(sanitized_list), '\\n')\n",
    "    return sanitized_list\n",
    "\n",
    "sample_list = [\"Shaquille O'neal\", \"J. J. Reddick\", \"VinCe Carter \", \"Bryant, Kobe\"]\n",
    "\n",
    "print(sanitize_list(sample_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "Remove 0 rows with \"None\" as their value.\n",
      "SAVED > 100 rows to spreadsheet: \"test_spreadsheet\" | worksheet: \"test\" | timestamp: 2017-11-21 13:21:22.724621 \n",
      "\n",
      "CPU times: user 144 ms, sys: 14.6 ms, total: 158 ms\n",
      "Wall time: 3.85 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def save_list_to_worksheet(lst, spreadsheet_name, worksheet_name, add_timestamp=True, overwrite=False):\n",
    "    scope = ['https://spreadsheets.google.com/feeds', 'https://www.googleapis.com/auth/drive']\n",
    "    credentials = ServiceAccountCredentials.from_json_keyfile_name('Data-35df9a696bc1.json', scope)\n",
    "    gc = gspread.authorize(credentials)\n",
    "    \n",
    "    spreadsheet = gc.open(spreadsheet_name)\n",
    "    \n",
    "    if type(lst) is not list:\n",
    "        print('ERROR: input item is not a list!')\n",
    "        return False\n",
    "    \n",
    "    # Remove rows with None value\n",
    "    original_length = len(lst)\n",
    "    lst = [item for item in lst if item is not None]\n",
    "    new_length = len(lst)\n",
    "    \n",
    "    try:\n",
    "        worksheet = spreadsheet.worksheet(worksheet_name)\n",
    "        if overwrite:\n",
    "            new_worksheet_name = worksheet_name + \"_new\"\n",
    "            new_worksheet = spreadsheet.add_worksheet(new_worksheet_name, len(lst), 1)\n",
    "            spreadsheet.del_worksheet(worksheet)\n",
    "            new_worksheet.update_title(worksheet_name)\n",
    "        else:\n",
    "            print('Worksheet \"{worksheet_name}\" already exist! Please set overwrite=True to overwrite.')\n",
    "            return False\n",
    "    except WorksheetNotFound: \n",
    "        new_worksheet = spreadsheet.add_worksheet(worksheet_name, len(lst), 1)\n",
    "    \n",
    "    range_notation = 'A1:A{last_row_index}'.format(last_row_index=len(lst))\n",
    "    \n",
    "    cells_to_update = new_worksheet.range(range_notation)\n",
    "\n",
    "    print('Remove {num_row} rows with \"None\" as their value.'.format(\n",
    "        num_row=(original_length - new_length)))\n",
    "    \n",
    "    for cell, item in zip(cells_to_update, lst):\n",
    "        cell.value = item\n",
    "    \n",
    "    new_worksheet.update_cells(cells_to_update)\n",
    "    \n",
    "    #Add a timestamp in the 1st cell\n",
    "    if add_timestamp:\n",
    "        timestamp = datetime.datetime.now()\n",
    "        new_worksheet.insert_row(\n",
    "            [timestamp], 1)\n",
    "    \n",
    "    print(\n",
    "    'SAVED > {num_rows} rows to '\n",
    "    'spreadsheet: \"{spreadsheet_name}\" | '\n",
    "    'worksheet: \"{worksheet_name}\" | '\n",
    "    'timestamp: {timestamp}'.format(\n",
    "        num_rows=len(lst), spreadsheet_name=spreadsheet_name, \n",
    "        worksheet_name=worksheet_name, timestamp=timestamp), '\\n')\n",
    "    \n",
    "    return True\n",
    "\n",
    "test_lst = ['michale', 'kobe'] + [i for i in range(98)]\n",
    "print(len(test_lst))\n",
    "save_list_to_worksheet(test_lst, 'test_spreadsheet', 'test', add_timestamp=True, overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save & Load Pickled Dictionaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SAVED  >  test.pickle  |  183.8 KiB  |  length:  5 \n",
      "\n",
      "\n",
      "LOADED >  test.pickle  |  183.8 KiB  |  length:  5 \n",
      "\n",
      "CPU times: user 8.39 ms, sys: 2.51 ms, total: 10.9 ms\n",
      "Wall time: 10.2 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "test = {\n",
    "    'words': \"\"\"\n",
    "        Lorem ipsum dolor sit amet, consectetur adipiscing \n",
    "        elit. Mauris adipiscing adipiscing placerat. \n",
    "        Vestibulum augue augue, \n",
    "        pellentesque quis sollicitudin id, adipiscing.\n",
    "        \"\"\",\n",
    "    'list': list(range(10000)),\n",
    "    'dict': dict((str(i),'a') for i in range(10000)),\n",
    "    'int': 100,\n",
    "    'float': 100.123456\n",
    "}\n",
    "\n",
    "def sizeof_fmt(num, suffix='B'):\n",
    "    for unit in ['','Ki','Mi','Gi','Ti','Pi','Ei','Zi']:\n",
    "        if abs(num) < 1024.0:\n",
    "            return \"%3.1f %s%s\" % (num, unit, suffix)\n",
    "        num /= 1024.0\n",
    "    return \"%.1f %s%s\" % (num, 'Yi', suffix)\n",
    "\n",
    "def get_file_size(filename):\n",
    "    statinfo = os.stat(filename)\n",
    "    return sizeof_fmt(statinfo.st_size)\n",
    "\n",
    "def save_pickle(dictionary, filename):\n",
    "    with open(filename, 'wb') as file:\n",
    "        pickle.dump(dictionary, file)\n",
    "    print(\n",
    "        '\\n'\n",
    "        'SAVED  > ',\n",
    "        filename, ' | ', \n",
    "        get_file_size(filename), ' | ',\n",
    "        'length: ', len(dictionary),\n",
    "        '\\n')\n",
    "    return True;\n",
    "\n",
    "def load_pickle(filename):\n",
    "    with open(filename, 'rb') as file:\n",
    "        obj = pickle.load(file)\n",
    "        print(\n",
    "        '\\n'\n",
    "        'LOADED > ',\n",
    "        filename, ' | ', \n",
    "        get_file_size(filename), ' | ',\n",
    "        'length: ', len(obj),\n",
    "        '\\n')\n",
    "        return obj\n",
    "\n",
    "save_pickle(test, 'test.pickle')\n",
    "\n",
    "len(load_pickle('test.pickle'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define & Load Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOADED > 181 rows from spreadsheet: \"nba_player_names_sanitized\" | worksheet: \"hof\" | timestamp: None \n",
      "\n",
      "List sanitized! Length:  181 \n",
      "\n",
      "LOADED > 352 rows from spreadsheet: \"nba_player_names_sanitized\" | worksheet: \"retired_all_stars\" | timestamp: None \n",
      "\n",
      "List sanitized! Length:  352 \n",
      "\n",
      "LOADED > 191 rows from spreadsheet: \"nba_player_names_sanitized\" | worksheet: \"retired_all_nbas\" | timestamp: None \n",
      "\n",
      "List sanitized! Length:  191 \n",
      "\n",
      "LOADED > 476 rows from spreadsheet: \"nba_player_names_sanitized\" | worksheet: \"2015\" | timestamp: None \n",
      "\n",
      "List sanitized! Length:  476 \n",
      "\n",
      "\n",
      "LOADED >  hof_urls.pickle  |  12.4 KiB  |  length:  181 \n",
      "\n",
      "\n",
      "LOADED >  retired_all_stars_urls.pickle  |  31.3 KiB  |  length:  352 \n",
      "\n",
      "\n",
      "LOADED >  retired_all_nbas_urls.pickle  |  16.6 KiB  |  length:  190 \n",
      "\n",
      "\n",
      "LOADED >  players_2015_urls.pickle  |  42.9 KiB  |  length:  476 \n",
      "\n",
      "CPU times: user 298 ms, sys: 51.6 ms, total: 350 ms\n",
      "Wall time: 3.85 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Tables to retrieve for each player, by table html ids\n",
    "table_ids = [\n",
    "  'per_game',\n",
    "  'totals',\n",
    "  'per_minute', # per 36 minutes\n",
    "  'per_poss', # per 100 possessions\n",
    "  'advanced', # advanced\n",
    "    \n",
    "  'playoffs_per_game',\n",
    "  'playoffs_totals',\n",
    "  'playoffs_per_minute', # playoffs per 36 minutes\n",
    "  'playoffs_per_poss', # playoffs per 100 possessions\n",
    "  'playoffs_advanced', \n",
    "    \n",
    "  'all_star',\n",
    "  'all_college_stats',\n",
    "  'all_salaries',\n",
    "]\n",
    "\n",
    "# Load player names\n",
    "hof_names = sanitize_list(\n",
    "    load_list_from_worksheet('nba_player_names_sanitized', 'hof')[0].tolist())\n",
    "retired_all_stars_names = sanitize_list(\n",
    "    load_list_from_worksheet('nba_player_names_sanitized', 'retired_all_stars')[0].tolist())\n",
    "retired_all_nbas_names = sanitize_list(\n",
    "    load_list_from_worksheet('nba_player_names_sanitized', 'retired_all_nbas')[0].tolist())\n",
    "players_2015_names = sanitize_list(\n",
    "    load_list_from_worksheet('nba_player_names_sanitized', '2015')[0].tolist())\n",
    "\n",
    "\n",
    "# Load URLs\n",
    "hof_urls = load_pickle('hof_urls.pickle')\n",
    "retired_all_stars_urls = load_pickle('retired_all_stars_urls.pickle')\n",
    "retired_all_nbas_urls = load_pickle('retired_all_nbas_urls.pickle')\n",
    "players_2015_urls = load_pickle('players_2015_urls.pickle')\n",
    "\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GET URL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get URL for a player name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8 µs, sys: 1 µs, total: 9 µs\n",
      "Wall time: 11.9 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#TODO(jameshu): Add logic to verify the url returned  in fact matches the player name\n",
    "# Currently, even gibberish player_name e.g. \"James Hu\" would have results returned.\n",
    "\n",
    "def get_url_title(url):\n",
    "    page = urllib.request.urlopen(url)\n",
    "    soup = BeautifulSoup(page, \"html.parser\")\n",
    "    return soup.title.text\n",
    "\n",
    "def get_url(player_name):       \n",
    "    query = (\n",
    "        'site:www.basketball-reference.com/players/*/*.html '\n",
    "        '{player_name} Overview').format(player_name=player_name)\n",
    "    print('query: ', query)\n",
    "\n",
    "    results = google.search(query=query, start=0, stop=1)\n",
    "    urls = list(results)        \n",
    "    \n",
    "    time.sleep(random.randint(5, 10))\n",
    "    \n",
    "    if urls:\n",
    "        return {player_name: urls[0]}\n",
    "    else:\n",
    "        print('url found: None')\n",
    "        return {player_name: None}\n",
    "        \n",
    "# print(get_url('Michael Jordan'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get URLs for a list of player names, MULTIPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5 µs, sys: 0 ns, total: 5 µs\n",
      "Wall time: 8.34 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def get_urls(player_names, num_processes):\n",
    "    p = multiprocessing.Pool(processes=num_processes)\n",
    "    outputs = p.map(get_url, player_names)\n",
    "    p.close()\n",
    "    return outputs\n",
    "\n",
    "# print(get_urls(test_names[0:2], 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GET TABLES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get stats table for an url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5 µs, sys: 0 ns, total: 5 µs\n",
      "Wall time: 6.91 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def get_table(url):\n",
    "    \n",
    "    output = {}\n",
    "    \n",
    "    page = urllib.request.urlopen(url)\n",
    "    urlHtml = page.read().decode()\n",
    "\n",
    "    # Get the player name\n",
    "    soup = BeautifulSoup(urlHtml, \"html.parser\")\n",
    "    player_name = soup.find(\"h1\").text\n",
    "\n",
    "    # Set the url\n",
    "    output.setdefault(player_name, {}).setdefault('url', url);\n",
    "\n",
    "    # Uncomment the tables\n",
    "    uncommentedUrlHtml = urlHtml.replace('-->', '')\n",
    "    uncommentedUrlHtml = uncommentedUrlHtml.replace('<!--', '')\n",
    "\n",
    "    for table_id in table_ids:\n",
    "        list_of_df = []\n",
    "        try:\n",
    "            list_of_df = pd.read_html(\n",
    "                uncommentedUrlHtml, \n",
    "                header=0, \n",
    "                attrs={'id': table_id})\n",
    "        except ValueError as err:\n",
    "            # Set missing_tables\n",
    "            output.setdefault(player_name, {}).setdefault('missing_tables', []).append(table_id)\n",
    "            continue;\n",
    "\n",
    "        # Drop 'Unnamed' columns\n",
    "        for df in list_of_df:\n",
    "          df.drop([col_name for col_name in df.columns if 'Unnamed' in col_name], axis=1, inplace=True)\n",
    "\n",
    "        # Set table\n",
    "        output.setdefault(player_name, {}).setdefault('tables', {}).update({table_id: list_of_df[0]})\n",
    "\n",
    "    # Print processing info\n",
    "    print(player_name, ' | ', url)\n",
    "    print('Tables Found: ', len(output[player_name].get('tables', {})), \n",
    "          ' | missing_tables: ', output[player_name].get('missing_tables', []))\n",
    "    print()\n",
    "\n",
    "    return output\n",
    "\n",
    "# table = get_table('https://www.basketball-reference.com/players/b/bellawa01.html')\n",
    "# print('obj length: ', len(table))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get stats tables for a list of urls, MULTIPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7 µs, sys: 1 µs, total: 8 µs\n",
      "Wall time: 11.2 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Utility function to merge retrived data tables into 1 dictionary.\n",
    "def merge_dict(list_of_dict):\n",
    "    merged_dict = {}\n",
    "    for dictionary in list_of_dict:\n",
    "        merged_dict.update(dictionary)\n",
    "    return merged_dict\n",
    "\n",
    "test_list = [\n",
    "    {'michael jordan': {'tables': {}, 'missing_tables': 'none', 'url': 'diety'}},\n",
    "    {'kobe bryant': {'tables': {}, 'missing_tables': 'none', 'url': 'godly'}},\n",
    "]\n",
    "\n",
    "# print(merge_dict(test_list).keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 39 µs, sys: 1 µs, total: 40 µs\n",
      "Wall time: 46.3 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from multiprocessing import Pool\n",
    "from functools import partial\n",
    "\n",
    "def get_tables(urls, num_processes):\n",
    "    pool = Pool(processes=num_processes)\n",
    "    outputs = pool.map(get_table, urls)\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return merge_dict(outputs)\n",
    "\n",
    "# tables = get_tables(test_urls, 2)\n",
    "# print('obj length: ', len(tables))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# worksheet = load_list_from_worksheet('nba_players_sanitized', 'hof')\n",
    "# hof_names = sanitize_list(worksheet[0].tolist())\n",
    "# print(hof_names)\n",
    "\n",
    "# hof_urls = get_urls(hof_names, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# worksheet = load_list_from_worksheet('nba_players_sanitized', 'retired_all_stars')\n",
    "# retired_all_stars_names = sanitize_list(worksheet[0].tolist())\n",
    "# print(retired_all_stars_names)\n",
    "\n",
    "# retired_all_stars_urls = get_urls(retired_all_stars_names, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# worksheet = load_list_from_worksheet('nba_players_sanitized', 'retired_all_nbas')\n",
    "# retired_all_nbas_names = sanitize_list(worksheet[0].tolist())\n",
    "# print(retired_all_nbas_names)\n",
    "\n",
    "# retired_all_nbas_urls = get_urls(retired_all_nbas_names, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# worksheet = load_list_from_worksheet('nba_players_sanitized', '2015')\n",
    "# players_2015_names = sanitize_list(worksheet[0].tolist())\n",
    "# print(players_2015_names)\n",
    "\n",
    "# players_2015_urls = get_urls(players_2015_names, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_pickle(hof_urls, 'hof_urls.pickle')\n",
    "# save_pickle(retired_all_stars_urls, 'retired_all_stars_urls.pickle')\n",
    "# save_pickle(retired_all_nbas_urls, 'retired_all_nbas_urls.pickle')\n",
    "# save_pickle(players_2015_urls, 'players_2015_urls.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remove 50 rows with \"None\" as their value.\n",
      "SAVED > 131 rows to spreadsheet: \"nba_player_urls\" | worksheet: \"hof_urls\" | timestamp: 2017-11-21 13:21:31.733691 \n",
      "\n",
      "Remove 0 rows with \"None\" as their value.\n",
      "SAVED > 190 rows to spreadsheet: \"nba_player_urls\" | worksheet: \"retired_all_nbas_urls\" | timestamp: 2017-11-21 13:21:35.894669 \n",
      "\n",
      "Remove 0 rows with \"None\" as their value.\n",
      "SAVED > 352 rows to spreadsheet: \"nba_player_urls\" | worksheet: \"retired_all_stars_urls\" | timestamp: 2017-11-21 13:21:40.841957 \n",
      "\n",
      "Remove 0 rows with \"None\" as their value.\n",
      "SAVED > 476 rows to spreadsheet: \"nba_player_urls\" | worksheet: \"players_2015_urls\" | timestamp: 2017-11-21 13:21:46.229754 \n",
      "\n",
      "CPU times: user 864 ms, sys: 89.2 ms, total: 953 ms\n",
      "Wall time: 19 s\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "\n",
    "# save_list_to_worksheet(list(hof_urls.values()), 'nba_player_urls', 'hof_urls', overwrite=True)\n",
    "# save_list_to_worksheet(list(retired_all_nbas_urls.values()), 'nba_player_urls', 'retired_all_nbas_urls', overwrite=True)\n",
    "# save_list_to_worksheet(list(retired_all_stars_urls.values()), 'nba_player_urls', 'retired_all_stars_urls', overwrite=True)\n",
    "# save_list_to_worksheet(list(players_2015_urls.values()), 'nba_player_urls', 'players_2015_urls', overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# url_list = [url for url in hof_urls.values() if url is not None]\n",
    "# print(len(url_list))\n",
    "# hof_tables = get_tables(url_list, 4)\n",
    "# save_pickle(hof_tables, 'hof_tables.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# url_list = [url for url in retired_all_nbas_urls.values() if url is not None]\n",
    "# print(len(url_list))\n",
    "# retired_all_nbas_tables = get_tables(url_list, 4)\n",
    "# save_pickle(retired_all_nbas_tables, 'retired_all_nbas_tables.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# url_list = [url for url in retired_all_stars_urls.values() if url is not None]\n",
    "# print(len(url_list))\n",
    "# retired_all_stars_tables = get_tables(url_list, 4)\n",
    "# save_pickle(retired_all_stars_tables, 'retired_all_stars_tables.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# url_list = [url for url in players_2015_urls.values() if url is not None]\n",
    "# print(len(url_list))\n",
    "# players_2015_tables = get_tables(url_list, 4)\n",
    "# save_pickle(players_2015_tables, 'players_2015_tables.pickle')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
