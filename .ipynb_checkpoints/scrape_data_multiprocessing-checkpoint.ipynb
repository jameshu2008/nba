{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Stats from Basketball-Reference.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2017-11\n",
    "\n",
    "Scrap data from basketball-reference.com, using pd.read_html, BeaultifulSoup, Multiprocessing, & Other python modules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports, Constants, Utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-24T17:24:24.468705Z",
     "start_time": "2017-11-24T17:24:23.808090Z"
    },
    "init_cell": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current TABLE_IDS length:  13\n",
      "CPU times: user 662 µs, sys: 517 µs, total: 1.18 ms\n",
      "Wall time: 1.04 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import datetime\n",
    "import re\n",
    "\n",
    "import json\n",
    "import pickle\n",
    "import urllib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import google\n",
    "import random\n",
    "import time\n",
    "import multiprocessing\n",
    "import gspread\n",
    "import unidecode\n",
    "import tqdm\n",
    "import pprint\n",
    "\n",
    "from collections import OrderedDict\n",
    "from gspread import WorksheetNotFound\n",
    "from bs4 import BeautifulSoup\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 100)\n",
    "\n",
    "\n",
    "DATETIME_STRING_FORMAT = '%Y-%m-%d %H:%M:%S'\n",
    "\n",
    "# Tables to retrieve for each player, by table html ids\n",
    "TABLE_IDS = [\n",
    "  'per_game',\n",
    "  'totals',\n",
    "  'per_minute', # per 36 minutes\n",
    "  'per_poss', # per 100 possessions\n",
    "  'advanced', # advanced\n",
    "    \n",
    "  'playoffs_per_game',\n",
    "  'playoffs_totals',\n",
    "  'playoffs_per_minute', # playoffs per 36 minutes\n",
    "  'playoffs_per_poss', # playoffs per 100 possessions\n",
    "  'playoffs_advanced', \n",
    "    \n",
    "  'all_star',\n",
    "  'all_college_stats',\n",
    "  'all_salaries',\n",
    "]\n",
    "\n",
    "print('Current TABLE_IDS length: ', len(TABLE_IDS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-24T17:24:32.349070Z",
     "start_time": "2017-11-24T17:24:32.341663Z"
    },
    "init_cell": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 0 ns, total: 3 µs\n",
      "Wall time: 4.77 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def merge_list_of_list(nested_list):\n",
    "    flattened_list = [item for lst in nested_list for item in lst]\n",
    "    return flattened_list\n",
    "\n",
    "test_list = [['a'], ['b']]\n",
    "merge_list_of_list(test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-24T17:24:33.572437Z",
     "start_time": "2017-11-24T17:24:33.559485Z"
    },
    "init_cell": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('kobe bryant', {'tables': {}, 'missing_tables': 'none', 'url': 'godly'}), ('michael jordan', {'tables': {}, 'missing_tables': 'none', 'url': 'diety'})])\n",
      "CPU times: user 133 µs, sys: 106 µs, total: 239 µs\n",
      "Wall time: 211 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Utility function to merge retrived data tables into 1 dictionary.\n",
    "def merge_list_of_dict(list_of_dict):\n",
    "    merged_dict = {}\n",
    "    for dictionary in list_of_dict:\n",
    "        merged_dict.update(dictionary)\n",
    "    # Sort by dictionary key\n",
    "    ordered_dict = OrderedDict(sorted(merged_dict.items(), key=lambda t: t[0]))\n",
    "    return ordered_dict\n",
    "\n",
    "test_list = [\n",
    "    {'michael jordan': {'tables': {}, 'missing_tables': 'none', 'url': 'diety'}},\n",
    "    {'kobe bryant': {'tables': {}, 'missing_tables': 'none', 'url': 'godly'}},\n",
    "]\n",
    "\n",
    "dic = merge_list_of_dict(test_list)\n",
    "print(dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-24T17:24:35.114219Z",
     "start_time": "2017-11-24T17:24:35.094764Z"
    },
    "init_cell": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shaquille oneal\n",
      "kobe bryant\n",
      "vince carter\n",
      "CPU times: user 458 µs, sys: 515 µs, total: 973 µs\n",
      "Wall time: 674 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def sanitize_string(raw_string):\n",
    "    sanitized_string = unidecode.unidecode(raw_string)\n",
    "    sanitized_string = sanitized_string.strip().lower()\n",
    "    sanitized_string = sanitized_string.replace(\"'\", \"\")\n",
    "    sanitized_string = sanitized_string.replace('\"', '') \n",
    "    sanitized_string = sanitized_string.replace('.', '')\n",
    "    if \",\" in sanitized_string:\n",
    "        lst = sanitized_string.split(\",\")\n",
    "        lst.reverse()\n",
    "        lst = [token.strip() for token in lst]\n",
    "        sanitized_string = \" \".join(lst)\n",
    "    return sanitized_string\n",
    "\n",
    "print(sanitize_string(\"Shaquille O'neal\"))\n",
    "print(sanitize_string(\"Bryant, Kobe\"))\n",
    "print(sanitize_string(\" CarTer, Vince ..\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-24T17:24:36.472210Z",
     "start_time": "2017-11-24T17:24:36.464700Z"
    },
    "init_cell": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['shaquille oneal', 'j j reddick', 'vince carter', 'kobe bryant']\n",
      "CPU times: user 90 µs, sys: 32 µs, total: 122 µs\n",
      "Wall time: 126 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def sanitize_list(raw_list):\n",
    "            \n",
    "    sanitized_list = [sanitize_string(raw_string) for raw_string in raw_list]\n",
    "    return sanitized_list\n",
    "\n",
    "test_list = [\"Shaquille O'neal\", \"J. J. Reddick\", \"VinCe Carter \", \"Bryant, Kobe\"]\n",
    "\n",
    "print(sanitize_list(test_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-24T17:24:37.791014Z",
     "start_time": "2017-11-24T17:24:37.778471Z"
    },
    "init_cell": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['b', 'a']\n",
      "CPU times: user 58 µs, sys: 27 µs, total: 85 µs\n",
      "Wall time: 89.2 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def dedupe_list(lst):\n",
    "    return list(set(lst))\n",
    "\n",
    "print(dedupe_list(['a', 'a', 'a', 'a', 'a', 'a', 'b', 'b', 'b', 'b', 'b', ]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Spreadsheets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-24T17:24:40.602802Z",
     "start_time": "2017-11-24T17:24:39.506605Z"
    },
    "init_cell": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOADED > 100 rows from spreadsheet: \"test_spreadsheet\" | worksheet: \"test\" | timestamp: 2017-11-24 09:29:12 \n",
      "\n",
      "['michale', 'kobe', '0', '1', '2', '3', '4', '5', '6', '7']\n"
     ]
    }
   ],
   "source": [
    "def load_list_from_worksheet(spreadsheet_name, worksheet_name):\n",
    "  \n",
    "    scope = ['https://spreadsheets.google.com/feeds']\n",
    "    credentials = ServiceAccountCredentials.from_json_keyfile_name('Data-35df9a696bc1.json', scope)\n",
    "    gc = gspread.authorize(credentials)\n",
    "\n",
    "    spreadsheet = gc.open(spreadsheet_name)\n",
    "    worksheet = spreadsheet.worksheet(worksheet_name)\n",
    "\n",
    "    rows = worksheet.get_all_values()\n",
    "    \n",
    "    first_row = rows[0]\n",
    "    first_cell = first_row[0]\n",
    "    \n",
    "    try:\n",
    "        timestamp = datetime.datetime.strptime(first_cell, DATETIME_STRING_FORMAT)\n",
    "        rows.remove(first_row)\n",
    "    except ValueError:\n",
    "        timestamp = None\n",
    "\n",
    "    print(\n",
    "        'LOADED > {num_rows} rows from '\n",
    "        'spreadsheet: \"{spreadsheet_name}\" | '\n",
    "        'worksheet: \"{worksheet_name}\" | '\n",
    "        'timestamp: {timestamp}'.format(\n",
    "            num_rows=len(rows), spreadsheet_name=spreadsheet_name, \n",
    "            worksheet_name=worksheet_name, timestamp=timestamp), '\\n')\n",
    "\n",
    "    df = merge_list_of_list(rows)\n",
    "    \n",
    "    return df\n",
    "\n",
    "worksheet = load_list_from_worksheet('test_spreadsheet', 'test')\n",
    "print(worksheet[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-24T17:24:46.124982Z",
     "start_time": "2017-11-24T17:24:41.268708Z"
    },
    "init_cell": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "Remove 0 rows with \"None\" as their value.\n",
      "SAVED > 100 rows to spreadsheet: \"test_spreadsheet\" | worksheet: \"test\" | timestamp: 2017-11-24 09:32:11.257191 \n",
      "\n",
      "CPU times: user 98.5 ms, sys: 14 ms, total: 112 ms\n",
      "Wall time: 3.55 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def save_list_to_worksheet(lst, spreadsheet_name, worksheet_name, add_timestamp=True, overwrite=False):\n",
    "    scope = ['https://spreadsheets.google.com/feeds', 'https://www.googleapis.com/auth/drive']\n",
    "    credentials = ServiceAccountCredentials.from_json_keyfile_name('Data-35df9a696bc1.json', scope)\n",
    "    gc = gspread.authorize(credentials)\n",
    "    \n",
    "    spreadsheet = gc.open(spreadsheet_name)\n",
    "    \n",
    "    if type(lst) is not list:\n",
    "        print('ERROR: input item is not a list!')\n",
    "        return False\n",
    "    \n",
    "    # Remove rows with None value\n",
    "    original_length = len(lst)\n",
    "    lst = [item for item in lst if item is not None]\n",
    "    new_length = len(lst)\n",
    "    \n",
    "    try:\n",
    "        worksheet = spreadsheet.worksheet(worksheet_name)\n",
    "        if overwrite:\n",
    "            new_worksheet_name = worksheet_name + \"_new\"\n",
    "            new_worksheet = spreadsheet.add_worksheet(new_worksheet_name, len(lst), 1)\n",
    "            spreadsheet.del_worksheet(worksheet)\n",
    "            new_worksheet.update_title(worksheet_name)\n",
    "        else:\n",
    "            print('Worksheet \"{worksheet_name}\" already exist! Please set overwrite=True to overwrite.')\n",
    "            return False\n",
    "    except WorksheetNotFound: \n",
    "        new_worksheet = spreadsheet.add_worksheet(worksheet_name, len(lst), 1)\n",
    "    \n",
    "    range_notation = 'A1:A{last_row_index}'.format(last_row_index=len(lst))\n",
    "    \n",
    "    cells_to_update = new_worksheet.range(range_notation)\n",
    "\n",
    "    print('Remove {num_row} rows with \"None\" as their value.'.format(\n",
    "        num_row=(original_length - new_length)))\n",
    "    \n",
    "    for cell, item in zip(cells_to_update, lst):\n",
    "        cell.value = item\n",
    "    \n",
    "    new_worksheet.update_cells(cells_to_update)\n",
    "    \n",
    "    #Add a timestamp in the 1st cell\n",
    "    if add_timestamp:\n",
    "        timestamp = datetime.datetime.now()\n",
    "        new_worksheet.insert_row(\n",
    "            [timestamp], 1)\n",
    "    \n",
    "    print(\n",
    "    'SAVED > {num_rows} rows to '\n",
    "    'spreadsheet: \"{spreadsheet_name}\" | '\n",
    "    'worksheet: \"{worksheet_name}\" | '\n",
    "    'timestamp: {timestamp}'.format(\n",
    "        num_rows=len(lst), spreadsheet_name=spreadsheet_name, \n",
    "        worksheet_name=worksheet_name, timestamp=timestamp), '\\n')\n",
    "    \n",
    "    return True\n",
    "\n",
    "test_lst = ['michale', 'kobe'] + [i for i in range(98)]\n",
    "print(len(test_lst))\n",
    "save_list_to_worksheet(test_lst, 'test_spreadsheet', 'test', add_timestamp=True, overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save & Load Pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-24T17:24:46.247056Z",
     "start_time": "2017-11-24T17:24:46.152831Z"
    },
    "init_cell": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SAVED  >  test.pickle  |  183.8 KiB  |  length:  5 \n",
      "\n",
      "\n",
      "LOADED >  test.pickle  |  183.8 KiB  |  length:  5 \n",
      "\n",
      "CPU times: user 10.5 ms, sys: 2 ms, total: 12.5 ms\n",
      "Wall time: 13.6 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "test = {\n",
    "    'words': \"\"\"\n",
    "        Lorem ipsum dolor sit amet, consectetur adipiscing \n",
    "        elit. Mauris adipiscing adipiscing placerat. \n",
    "        Vestibulum augue augue, \n",
    "        pellentesque quis sollicitudin id, adipiscing.\n",
    "        \"\"\",\n",
    "    'list': list(range(10000)),\n",
    "    'dict': dict((str(i),'a') for i in range(10000)),\n",
    "    'int': 100,\n",
    "    'float': 100.123456\n",
    "}\n",
    "\n",
    "def sizeof_fmt(num, suffix='B'):\n",
    "    for unit in ['','Ki','Mi','Gi','Ti','Pi','Ei','Zi']:\n",
    "        if abs(num) < 1024.0:\n",
    "            return \"%3.1f %s%s\" % (num, unit, suffix)\n",
    "        num /= 1024.0\n",
    "    return \"%.1f %s%s\" % (num, 'Yi', suffix)\n",
    "\n",
    "def get_file_size(filename):\n",
    "    statinfo = os.stat(filename)\n",
    "    return sizeof_fmt(statinfo.st_size)\n",
    "\n",
    "def save_pickle(item, filename):\n",
    "    with open(filename, 'wb') as file:\n",
    "        pickle.dump(item, file)\n",
    "    print(\n",
    "        '\\n'\n",
    "        'SAVED  > ',\n",
    "        filename, ' | ', \n",
    "        get_file_size(filename), ' | ',\n",
    "        'length: ', len(item),\n",
    "        '\\n')\n",
    "    return True;\n",
    "\n",
    "def load_pickle(filename):\n",
    "    with open(filename, 'rb') as file:\n",
    "        obj = pickle.load(file)\n",
    "        print(\n",
    "        '\\n'\n",
    "        'LOADED > ',\n",
    "        filename, ' | ', \n",
    "        get_file_size(filename), ' | ',\n",
    "        'length: ', len(obj),\n",
    "        '\\n')\n",
    "        return obj\n",
    "\n",
    "save_pickle(test, 'test.pickle')\n",
    "\n",
    "len(load_pickle('test.pickle'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reshape & create dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-24T18:57:48.931547Z",
     "start_time": "2017-11-24T18:56:28.962252Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LOADED >  all_players_stats_tables.pickle  |  97.6 MiB  |  length:  3997 \n",
      "\n",
      "\n",
      "LOADED >  all_players_attributes.pickle  |  5.8 MiB  |  length:  3997 \n",
      "\n",
      "(3997, 19)\n",
      "\n",
      "SAVED  >  all_players_dfs.pickle  |  51.5 MiB  |  length:  14 \n",
      "\n",
      "# of Dataframes created:  14\n",
      "per_game (30982, 29)\n",
      "totals (29804, 29)\n",
      "per_minute (29804, 28)\n",
      "per_poss (24489, 30)\n",
      "advanced (29804, 28)\n",
      "playoffs_per_game (12319, 29)\n",
      "playoffs_totals (12319, 29)\n",
      "playoffs_per_minute (12319, 28)\n",
      "playoffs_per_poss (9756, 30)\n",
      "playoffs_advanced (12319, 26)\n",
      "all_star (2154, 24)\n",
      "all_college_stats (14251, 3)\n",
      "all_salaries (15807, 3)\n",
      "all_players_attributes (3997, 19)\n",
      "CPU times: user 1min 12s, sys: 1.24 s, total: 1min 14s\n",
      "Wall time: 1min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "all_players_stats_tables = load_pickle('all_players_stats_tables.pickle')\n",
    "all_players_attributes = load_pickle('all_players_attributes.pickle')\n",
    "\n",
    "all_players_attributes_df = pd.DataFrame.from_dict(all_players_attributes, orient='index')\n",
    "print(all_players_attributes_df.shape)\n",
    "\n",
    "all_players_dfs = {}\n",
    "for table_id in TABLE_IDS:\n",
    "    l_dfs = []\n",
    "    l_keys = []\n",
    "    for name, player in all_players_stats_tables.items():\n",
    "        df = player.get('tables', {}).get(table_id, None)\n",
    "        if df is not None:\n",
    "            l_dfs.append(df)\n",
    "            l_keys.append(name)\n",
    "    all_players_dfs[table_id] = pd.concat(l_dfs, keys=l_keys)\n",
    "\n",
    "all_players_dfs['all_players_attributes'] = all_players_attributes_df\n",
    "\n",
    "save_pickle(all_players_dfs, 'all_players_dfs.pickle')\n",
    "\n",
    "print('# of Dataframes created: ', len(all_players_dfs))\n",
    "for key in all_players_dfs.keys():\n",
    "    print(key, all_players_dfs[key].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Players"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get players from a single URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-24T17:20:14.107970Z",
     "start_time": "2017-11-24T17:20:07.690499Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_players(url):\n",
    "    \n",
    "    href_pattern = re.compile('^\\/players\\/.\\/[a-z0-9]*.html$')\n",
    "    href_prefix = 'https://www.basketball-reference.com'\n",
    "    \n",
    "    page = urllib.request.urlopen(url)\n",
    "    html = page.read()\n",
    "\n",
    "    # Get the player name\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "    els = soup.find_all('a', href=href_pattern)\n",
    "    \n",
    "    players = {}\n",
    "    \n",
    "    for el in els:\n",
    "        if el.parent.name == 'td':\n",
    "            player_name = sanitize_string(el.text)\n",
    "            player_url = ''.join([href_prefix, el['href']])\n",
    "            players[player_name] = player_url\n",
    "    \n",
    "    randomized_sleep_time = 5 + np.random.exponential(1, 1)[0]\n",
    "    time.sleep(randomized_sleep_time)\n",
    "    \n",
    "    print('Scrapped {url} | Players Found: {len}'.format(url=url, len=len(players)))\n",
    "    sys.stdout.flush()\n",
    "\n",
    "    return players\n",
    "\n",
    "players = get_players('https://www.basketball-reference.com/leagues/NBA_1967_totals.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get players from a list of URLs, Multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-24T17:20:22.577311Z",
     "start_time": "2017-11-24T17:20:14.110596Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def get_players_from_urls(urls, num_processes):\n",
    "    p = multiprocessing.Pool(processes=num_processes)\n",
    "    outputs = p.map(get_players, urls)\n",
    "    p.close()\n",
    "    final_output = merge_list_of_dict(outputs)\n",
    "    print(\n",
    "        'Scrapped {num_url} urls, found {num_player} players.'.format(\n",
    "            num_url=len(urls), num_player=len(final_output)), '\\n')\n",
    "    return final_output\n",
    "\n",
    "test_urls = [\n",
    "    'https://www.basketball-reference.com/leagues/NBA_2015_totals.html',\n",
    "    'https://www.basketball-reference.com/leagues/NBA_2010_totals.html'\n",
    "]\n",
    "\n",
    "players = get_players_from_urls(test_urls, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GET URL (DEPRECATED, replaced by \"Get Players\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get URL for a player name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# #TODO(jameshu): Add logic to verify the url returned  in fact matches the player name\n",
    "# # Currently, even gibberish player_name e.g. \"James Hu\" would have results returned.\n",
    "\n",
    "# def get_url_title(url):\n",
    "#     page = urllib.request.urlopen(url)\n",
    "#     soup = BeautifulSoup(page, \"html.parser\")\n",
    "#     return soup.title.text\n",
    "\n",
    "# def get_url(player_name):       \n",
    "#     query = (\n",
    "#         'site:www.basketball-reference.com/players/*/*.html '\n",
    "#         '{player_name} Overview').format(player_name=player_name)\n",
    "#     print('query: ', query)\n",
    "\n",
    "#     results = google.search(query=query, start=0, stop=1)\n",
    "#     urls = list(results)        \n",
    "    \n",
    "#     time.sleep(random.randint(5, 10))\n",
    "    \n",
    "#     if urls:\n",
    "#         return {player_name: urls[0]}\n",
    "#     else:\n",
    "#         print('url found: None')\n",
    "#         return {player_name: None}\n",
    "        \n",
    "# # print(get_url('Michael Jordan'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get URLs for a list of player names, MULTIPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# def get_urls(player_names, num_processes):\n",
    "#     p = multiprocessing.Pool(processes=num_processes)\n",
    "#     outputs = p.map(get_url, player_names)\n",
    "#     p.close()\n",
    "#     return merge_list_of_dict(outputs)\n",
    "\n",
    "# # print(get_urls(test_names[0:2], 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get stats tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get stats table for an url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Test out the sleep function\n",
    "\n",
    "# mu, sigma = 0, 1\n",
    "# s = np.random.normal(mu, sigma, 1000)\n",
    "# pd.Series(s).hist()\n",
    "\n",
    "# mu, sigma = 0, 1\n",
    "# s = np.random.exponential(1, 100000)\n",
    "# pd.Series(s).hist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-24T17:20:29.036513Z",
     "start_time": "2017-11-24T17:20:22.582122Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def get_stats_table(url):\n",
    "        \n",
    "    output = {}\n",
    "    \n",
    "    page = urllib.request.urlopen(url)\n",
    "    urlHtml = page.read().decode()\n",
    "    \n",
    "    # Uncomment the tables\n",
    "    uncommentedUrlHtml = urlHtml.replace('-->', '')\n",
    "    uncommentedUrlHtml = uncommentedUrlHtml.replace('<!--', '')\n",
    "\n",
    "    soup = BeautifulSoup(uncommentedUrlHtml, 'lxml')\n",
    "    \n",
    "    player_name = sanitize_string(soup.find(\"h1\").text)\n",
    "    output.setdefault(player_name, {}).setdefault('url', url);\n",
    "\n",
    "    tags = soup.find_all('table')\n",
    "    \n",
    "    tables = {}\n",
    "    missing_table_ids = list(TABLE_IDS) # MAKE A COPY\n",
    "    \n",
    "    for tag in tags:\n",
    "        table_id = tag.get('id')\n",
    "        if table_id in TABLE_IDS:\n",
    "            table = pd.read_html(str(tag), header=0, index_col=0)[0]\n",
    "            tables[table_id] = table\n",
    "            missing_table_ids.remove(table_id)\n",
    "\n",
    "    for dataframe in tables.values():\n",
    "        dataframe.drop([col_name for col_name in dataframe.columns if 'Unnamed' in col_name], axis=1, inplace=True)\n",
    "       \n",
    "    output[player_name].setdefault('tables', tables);\n",
    "    output[player_name].setdefault('missing_tables', missing_table_ids);\n",
    "    \n",
    "    randomized_sleep_time = 5 + np.random.exponential(1, 1)[0]\n",
    "    time.sleep(randomized_sleep_time)\n",
    "    \n",
    "    processing_info = (\n",
    "        '{player_name} | Found: {num_table} | '\n",
    "        'slept: {randomized_sleep_time}'.format(\n",
    "            player_name=player_name, \n",
    "            num_table=len(output[player_name]['tables']), \n",
    "            missing_tables=missing_table_ids,\n",
    "            randomized_sleep_time=randomized_sleep_time))\n",
    "    \n",
    "    print(processing_info)\n",
    "    sys.stdout.flush()\n",
    "\n",
    "    return output\n",
    "\n",
    "table = get_stats_table('https://www.basketball-reference.com/players/b/bellawa01.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "table['walt bellamy']['tables']['totals']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "table['walt bellamy']['missing_tables']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "table['walt bellamy']['url']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get stats tables for a list of urls, MULTIPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-24T17:20:37.569910Z",
     "start_time": "2017-11-24T17:20:29.039843Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def get_stats_tables(urls, num_processes):\n",
    "    pool = multiprocessing.Pool(processes=num_processes)\n",
    "    jobs = pool.imap_unordered(get_stats_table, urls)\n",
    "    size = len(urls)\n",
    "    outputs = tqdm.tqdm_notebook(jobs, total=size)\n",
    "    pool.close()\n",
    "#     pool.join()\n",
    "    return merge_list_of_dict(outputs)\n",
    "\n",
    "test_urls = [\n",
    "    'https://www.basketball-reference.com/players/b/bellawa01.html',\n",
    "    'https://www.basketball-reference.com/players/j/jordami01.html'\n",
    "]\n",
    "\n",
    "tables = get_stats_tables(test_urls, 2)\n",
    "print('obj length: ', len(tables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tables['michael jordan']['tables']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Player Attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get a player's attributes from an URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-24T17:20:45.125461Z",
     "start_time": "2017-11-24T17:20:37.574124Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "def get_player_attributes(url):\n",
    "\n",
    "    output = {}\n",
    "    \n",
    "    try:\n",
    "\n",
    "        page = urllib.request.urlopen(url)\n",
    "        urlHtml = page.read().decode()\n",
    "\n",
    "        # Uncomment the tables\n",
    "        uncommentedUrlHtml = urlHtml.replace('-->', '')\n",
    "        uncommentedUrlHtml = uncommentedUrlHtml.replace('<!--', '')\n",
    "\n",
    "        soup = BeautifulSoup(uncommentedUrlHtml, 'lxml')\n",
    "\n",
    "        player_name = sanitize_string(soup.find(\"h1\").text)\n",
    "        output.setdefault(player_name, {}).setdefault('url', url)\n",
    "        output.setdefault(player_name, {}).setdefault('missing_attributes', [])\n",
    "\n",
    "        # Get all info, for future extraction\n",
    "        tag = soup.find('div', attrs={'id': 'info'})\n",
    "        if tag:\n",
    "            player_info = tag\n",
    "#             output['player_info_raw'] = tag\n",
    "        else:\n",
    "            output[player_name]['missing_attributes'].append('player_info_raw')\n",
    "\n",
    "        tag = player_info.find('strong',text=re.compile('.*Position:.*'))\n",
    "        if tag:\n",
    "            position = tag.parent.contents[2]\n",
    "            position = position.replace('\\n','').replace('▪','').strip()\n",
    "            position = re.sub('\\s{2,}', ' ', position)\n",
    "            shooting_hand = tag.parent.contents[-1]\n",
    "            shooting_hand = shooting_hand.replace('\\n','').replace('▪','').strip()\n",
    "            shooting_hand = re.sub('\\s{2,}', ' ', shooting_hand)\n",
    "            output[player_name]['position'] = position\n",
    "            output[player_name]['shooting_hand'] = shooting_hand\n",
    "        else:\n",
    "            output[player_name]['missing_attributes'].extend(['position', 'shooting_hand'])\n",
    "\n",
    "        tag = player_info.find('strong',text=re.compile('.*High School:.*'))\n",
    "        if tag:\n",
    "            school = tag.parent.text.replace('\\n', '').split(':')\n",
    "            school = school[-1].strip()\n",
    "            school = re.sub('\\s{2,}', ' ', school)\n",
    "            output[player_name]['high_school'] = school\n",
    "        else:\n",
    "            output[player_name]['missing_attributes'].append('high_school')\n",
    "\n",
    "        tag = player_info.find('strong',text=re.compile('.*College:.*'))\n",
    "        if tag:\n",
    "            tag = tag.parent.find('a')\n",
    "            if tag:\n",
    "                college = tag.text.strip()\n",
    "                output[player_name]['college'] = college\n",
    "            else: \n",
    "                output[player_name]['missing_attributes'].append('college')\n",
    "        else:\n",
    "            output[player_name]['missing_attributes'].append('college')\n",
    "\n",
    "        tag = player_info.find('strong',text=re.compile('.*Recruiting Rank:.*'))\n",
    "        if tag:\n",
    "            recruiting_rank = tag.parent.text.strip()\n",
    "            recruiting_rank = re.search('\\(([0-9]*)\\)', recruiting_rank).group(1)\n",
    "            output[player_name]['recruiting_rank'] = recruiting_rank\n",
    "        else:\n",
    "            output[player_name]['missing_attributes'].append('recruiting_rank')\n",
    "\n",
    "        tag = player_info.find('strong',text=re.compile('.*Draft:.*'))\n",
    "        if tag:\n",
    "            draft = tag.parent.text.replace('\\n', '').split(':')\n",
    "            draft = draft[-1].strip()\n",
    "            draft = re.sub('\\s{2,}', ' ', draft)\n",
    "            output[player_name]['draft'] = draft\n",
    "        else:\n",
    "            output[player_name]['missing_attributes'].append('draft')\n",
    "\n",
    "        tag = player_info.find('strong',text=re.compile('.*Debut:.*'))\n",
    "        if tag:\n",
    "            nba_debute = tag.parent.contents[2]\n",
    "            output[player_name]['nba_debut'] = nba_debute.text.strip()\n",
    "        else:\n",
    "            output[player_name]['missing_attributes'].append('nba_debut')            \n",
    "\n",
    "        href_pattern = re.compile('^https://twitter.com/.*$')\n",
    "        tag = player_info.find('a', href=href_pattern)\n",
    "        if tag:\n",
    "            output[player_name]['twitter'] = tag['href'].strip()\n",
    "        else:\n",
    "            output[player_name]['missing_attributes'].append('twitter')  \n",
    "\n",
    "        tag = player_info.find('span', attrs={'itemprop': 'birthDate'})\n",
    "        if tag:\n",
    "            output[player_name]['birth_date'] = tag['data-birth'].strip()\n",
    "        else:\n",
    "            output[player_name]['missing_attributes'].append('birth_date')  \n",
    "\n",
    "        tag = player_info.find('span', attrs={'itemprop': 'birthPlace'})\n",
    "        if tag:\n",
    "            tag = tag.find('a')\n",
    "            if tag:\n",
    "                output[player_name]['birth_place'] = tag.text.strip()\n",
    "            else:\n",
    "                output[player_name]['missing_attributes'].append('birth_place')\n",
    "        else:\n",
    "            output[player_name]['missing_attributes'].append('birth_place')  \n",
    "\n",
    "        tag = player_info.find('span', attrs={'itemprop': 'height'})\n",
    "        if tag:\n",
    "            output[player_name]['height'] = tag.text.strip()\n",
    "        else:\n",
    "            output[player_name]['missing_attributes'].append('height')  \n",
    "\n",
    "        tag = player_info.find('span', attrs={'itemprop': 'weight'})\n",
    "        if tag:\n",
    "            output[player_name]['weight'] = tag.text.strip()\n",
    "        else:\n",
    "            output[player_name]['missing_attributes'].append('weight')  \n",
    "\n",
    "        tags = player_info.find('ul', attrs={'id': 'bling'})\n",
    "        if tags:\n",
    "            tags = tags.find_all('a')\n",
    "            if tags:\n",
    "                output[player_name]['honors'] = []\n",
    "                for tag in tags:\n",
    "                    output[player_name]['honors'].append(tag.text.strip())\n",
    "            else:\n",
    "                output[player_name]['missing_attributes'].append('honors')\n",
    "        else:\n",
    "            output[player_name]['missing_attributes'].append('honors')  \n",
    "\n",
    "        tag = soup.find('p', text=re.compile('.*Chinese:.*'))\n",
    "        if tag:\n",
    "            chinese_name = tag.text.split(':')[-1].replace('數據','').strip()\n",
    "            output[player_name]['chinese_name'] = chinese_name\n",
    "        else:\n",
    "            output[player_name]['missing_attributes'].append('chinese_name')  \n",
    "\n",
    "        tags = soup.find_all('p', attrs={'class': 'transaction '})\n",
    "        if tags:\n",
    "            for tag in tags:\n",
    "                transaction_date = tag.find('strong').text.strip()\n",
    "                transaction = tag.text.split(':')[-1].strip()\n",
    "                transaction = re.sub('\\s{2,}', ' ', transaction)\n",
    "                output[player_name].setdefault('transactions', {})[transaction_date] = transaction\n",
    "        else:\n",
    "            output[player_name]['missing_attributes'].append('transactions')  \n",
    "\n",
    "        tag = player_info.find('p', text=re.compile('.*\\(.*\\).*'))\n",
    "        if tag:\n",
    "            nicknames = tag.text.replace('\\n','').split(',')\n",
    "            nicknames = [nickname.replace('(','').replace(')','').strip() for nickname in nicknames]\n",
    "            output[player_name]['nicknames'] = nicknames\n",
    "        else:\n",
    "            output[player_name]['missing_attributes'].append('nicknames')  \n",
    "\n",
    "        tags = player_info.find_all('svg', attrs={'class': 'jersey'})\n",
    "        if tags:\n",
    "            for tag in tags:\n",
    "                jersey_number = tag.find('text').text.strip()\n",
    "                team = tag.parent['data-tip'].strip()\n",
    "                output[player_name].setdefault('numbers', {})[jersey_number] = team\n",
    "        else:\n",
    "            output[player_name]['missing_attributes'].append('numbers')  \n",
    "\n",
    "\n",
    "        randomized_sleep_time = 5 + np.random.exponential(1, 1)[0]\n",
    "        time.sleep(randomized_sleep_time)\n",
    "\n",
    "        processing_info = (\n",
    "            '{player_name} | Missing: {num_missing} | '\n",
    "            'slept: {randomized_sleep_time}'.format(\n",
    "                player_name=player_name, \n",
    "                num_missing=len(output[player_name]['missing_attributes']), \n",
    "                randomized_sleep_time=randomized_sleep_time))\n",
    "\n",
    "        print(processing_info)\n",
    "        sys.stdout.flush()\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(url, \" FAILED! | \", str(e))\n",
    "        output[url] = e\n",
    "                \n",
    "    return output\n",
    "\n",
    "# test_url = 'https://www.basketball-reference.com/players/m/mingya01.html'\n",
    "# test_url = \"https://www.basketball-reference.com/players/b/bellawa01.html\"\n",
    "# test_url = 'https://www.basketball-reference.com/players/b/bryanko01.html'\n",
    "# test_url = 'https://www.basketball-reference.com/players/r/redicjj01.html'\n",
    "# test_url = 'https://www.basketball-reference.com/players/n/novakst01.html'\n",
    "# test_url = 'https://www.basketball-reference.com/players/j/jordami01.html'\n",
    "# test_url = 'https://www.basketball-reference.com/players/h/hairsal01.html'\n",
    "test_url = 'https://www.basketball-reference.com/players/h/henryal01.html'\n",
    "    \n",
    "test_output = get_player_attributes(test_url)\n",
    "pprint.pprint(test_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-24T17:21:00.846146Z",
     "start_time": "2017-11-24T17:20:45.127680Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def get_players_attributes(urls, num_processes):\n",
    "    pool = multiprocessing.Pool(processes=num_processes)\n",
    "    jobs = pool.imap_unordered(get_player_attributes, urls)\n",
    "    size = len(urls)\n",
    "    outputs = tqdm.tqdm_notebook(jobs, total=size)\n",
    "    pool.close()\n",
    "#     pool.join()\n",
    "    return merge_list_of_dict(outputs)\n",
    "\n",
    "test_urls = [\n",
    "    'https://www.basketball-reference.com/players/b/bellawa01.html',\n",
    "    'https://www.basketball-reference.com/players/j/jordami01.html',\n",
    "    'https://www.basketball-reference.com/players/n/novakst01.html',\n",
    "    'https://www.basketball-reference.com/players/m/mingya01.html',\n",
    "    'https://www.basketball-reference.com/players/b/bryanko01.html',\n",
    "    'https://www.basketball-reference.com/players/r/redicjj01.html'\n",
    "]\n",
    "\n",
    "attributes = get_players_attributes(test_urls, 4)\n",
    "print('obj length: ', len(tables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-24T18:54:03.861345Z",
     "start_time": "2017-11-24T18:54:03.491943Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SAVED  >  all_players_dfs.pickle  |  51.5 MiB  |  length:  14 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_pickle(all_players_dfs, 'all_players_dfs.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Burner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-11-24T17:25:30.673Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a_attribute_1</th>\n",
       "      <th>a_attribute_2</th>\n",
       "      <th>a_attribute_3</th>\n",
       "      <th>a_attribute_4</th>\n",
       "      <th>b_attribute_1</th>\n",
       "      <th>b_attribute_2</th>\n",
       "      <th>b_attribute_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>adfa</td>\n",
       "      <td>{}</td>\n",
       "      <td>[a, b, c]</td>\n",
       "      <td>123</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>erqwerdfa</td>\n",
       "      <td>{}</td>\n",
       "      <td>[a, b, c]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  a_attribute_1 a_attribute_2 a_attribute_3 a_attribute_4 b_attribute_1  \\\n",
       "a          adfa            {}     [a, b, c]           123           NaN   \n",
       "b           NaN           NaN           NaN           NaN     erqwerdfa   \n",
       "\n",
       "  b_attribute_2 b_attribute_3  \n",
       "a           NaN           NaN  \n",
       "b            {}     [a, b, c]  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_dict = {\n",
    "    \"a\": {\"a_attribute_1\": \"adfa\", \"a_attribute_2\": {}, \"a_attribute_3\": ['a', 'b', 'c'], \"a_attribute_4\": '123'},\n",
    "    \"b\": {\"b_attribute_1\": \"erqwerdfa\", \"b_attribute_2\": {}, \"b_attribute_3\": ['a', 'b', 'c']}\n",
    "}\n",
    "\n",
    "pd.DataFrame.from_dict(sample_dict, orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# all_players = load_pickle('all_players.pickle')\n",
    "# all_players_attributes = get_players_attributes(list(all_players.values()), 4)\n",
    "# save_pickle(all_players_attributes, 'all_players_attributes.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# all_players_stats_tables = get_stats_tables(list(all_players.values()), 4)\n",
    "\n",
    "# save_pickle(all_players_stats_tables, 'all_players_stats_tables.pickle')\n",
    "\n",
    "# print(all_players_stats_tables['lebron james']['tables'].keys())\n",
    "# all_players_stats_tables['lebron james']['tables']['per_game']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# leads_urls = load_list_from_worksheet('nba_player_names', 'leads_urls')\n",
    "\n",
    "# all_players = get_players_from_urls(leads_urls, 4)\n",
    "\n",
    "# save_pickle(all_players, 'all_players.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# worksheet = load_list_from_worksheet('nba_players_sanitized', 'hof')\n",
    "# hof_names = sanitize_list(worksheet[0].tolist())\n",
    "# print(hof_names)\n",
    "\n",
    "# hof_urls = get_urls(hof_names, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# worksheet = load_list_from_worksheet('nba_players_sanitized', 'retired_all_stars')\n",
    "# retired_all_stars_names = sanitize_list(worksheet[0].tolist())\n",
    "# print(retired_all_stars_names)\n",
    "\n",
    "# retired_all_stars_urls = get_urls(retired_all_stars_names, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# worksheet = load_list_from_worksheet('nba_players_sanitized', 'retired_all_nbas')\n",
    "# retired_all_nbas_names = sanitize_list(worksheet[0].tolist())\n",
    "# print(retired_all_nbas_names)\n",
    "\n",
    "# retired_all_nbas_urls = get_urls(retired_all_nbas_names, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# worksheet = load_list_from_worksheet('nba_players_sanitized', '2015')\n",
    "# players_2015_names = sanitize_list(worksheet[0].tolist())\n",
    "# print(players_2015_names)\n",
    "\n",
    "# players_2015_urls = get_urls(players_2015_names, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save_pickle(hof_urls, 'hof_urls.pickle')\n",
    "# save_pickle(retired_all_stars_urls, 'retired_all_stars_urls.pickle')\n",
    "# save_pickle(retired_all_nbas_urls, 'retired_all_nbas_urls.pickle')\n",
    "# save_pickle(players_2015_urls, 'players_2015_urls.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# save_list_to_worksheet(list(hof_urls.values()), 'nba_player_urls', 'hof_urls', overwrite=True)\n",
    "# save_list_to_worksheet(list(retired_all_nbas_urls.values()), 'nba_player_urls', 'retired_all_nbas_urls', overwrite=True)\n",
    "# save_list_to_worksheet(list(retired_all_stars_urls.values()), 'nba_player_urls', 'retired_all_stars_urls', overwrite=True)\n",
    "# save_list_to_worksheet(list(players_2015_urls.values()), 'nba_player_urls', 'players_2015_urls', overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# url_list = [url for url in hof_urls.values() if url is not None]\n",
    "# print(len(url_list))\n",
    "# hof_tables = get_tables(url_list, 4)\n",
    "# save_pickle(hof_tables, 'hof_tables.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# url_list = [url for url in retired_all_nbas_urls.values() if url is not None]\n",
    "# print(len(url_list))\n",
    "# retired_all_nbas_tables = get_tables(url_list, 4)\n",
    "# save_pickle(retired_all_nbas_tables, 'retired_all_nbas_tables.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# url_list = [url for url in retired_all_stars_urls.values() if url is not None]\n",
    "# print(len(url_list))\n",
    "# retired_all_stars_tables = get_tables(url_list, 4)\n",
    "# save_pickle(retired_all_stars_tables, 'retired_all_stars_tables.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# url_list = [url for url in players_2015_urls.values() if url is not None]\n",
    "# print(len(url_list))\n",
    "# players_2015_tables = get_tables(url_list, 4)\n",
    "# save_pickle(players_2015_tables, 'players_2015_tables.pickle')"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Initialization Cell",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
